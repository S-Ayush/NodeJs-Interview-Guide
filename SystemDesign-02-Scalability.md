# Chapter 2: Scalability & Performance

## 2.1 Understanding Scalability

Scalability is the capability of a system to handle increased load by adding resources. For senior engineers, it's not just about "making things faster"â€”it's about designing systems that gracefully handle 10x, 100x, or even 1000x growth.

### The Scalability Mindset

```
Current: 1,000 users
Question: "How will this work with 1,000,000 users?"

Current: 100 requests/second
Question: "What breaks at 10,000 requests/second?"

Current: 10 GB database
Question: "How do we handle 10 TB?"
```

### Scalability vs Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Performance: How fast for single user               â”‚
â”‚ Example: API responds in 100ms                      â”‚
â”‚                                                     â”‚
â”‚ Scalability: How system handles growing load       â”‚
â”‚ Example: API still responds in 100ms with          â”‚
â”‚          1000x more users                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Important**: A fast system isn't necessarily scalable!

```
Example: Single high-performance server
- Handles 1000 req/s with 50ms latency âœ“ Fast!
- Cannot handle 100,000 req/s âœ— Not scalable!

Better: Distributed system with load balancer
- Each server handles 1000 req/s with 100ms latency
- Can scale to 100 servers â†’ 100,000 req/s âœ“ Scalable!
```

---

## 2.2 Vertical vs Horizontal Scaling

### Vertical Scaling (Scale Up)

**Definition**: Adding more resources (CPU, RAM, SSD) to existing server.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Before                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  Server             â”‚           â”‚
â”‚   â”‚  4 CPU cores        â”‚           â”‚
â”‚   â”‚  8 GB RAM           â”‚           â”‚
â”‚   â”‚  100 GB SSD         â”‚           â”‚
â”‚   â”‚                     â”‚           â”‚
â”‚   â”‚  Handles:           â”‚           â”‚
â”‚   â”‚  1000 req/s         â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                 â†“ Scale Up

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           After                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  Server             â”‚           â”‚
â”‚   â”‚  16 CPU cores   â†‘   â”‚           â”‚
â”‚   â”‚  64 GB RAM      â†‘   â”‚           â”‚
â”‚   â”‚  1 TB SSD       â†‘   â”‚           â”‚
â”‚   â”‚                     â”‚           â”‚
â”‚   â”‚  Handles:           â”‚           â”‚
â”‚   â”‚  4000 req/s         â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- âœ… Simple to implement (no code changes)
- âœ… No distributed system complexity
- âœ… Maintains data consistency
- âœ… Lower latency (no network calls)

**Cons:**
- âŒ Hard limit (can't scale infinitely)
- âŒ Expensive (costs grow exponentially)
- âŒ Downtime during upgrades
- âŒ Single point of failure
- âŒ Geographic limitations

**When to Use:**
- Early stage systems (< 10,000 users)
- Databases (before sharding)
- Monolithic applications
- When complexity isn't worth it yet

**Real-World Example: Stack Overflow**

Stack Overflow famously runs on just a few powerful servers:
```
2013 Architecture:
- 2 web servers (11 million page views/month)
- 2 SQL servers
- 1 Redis server

Why vertical scaling works for them:
âœ“ Mature, optimized codebase
âœ“ Heavy caching (hit rate > 95%)
âœ“ Not rapidly growing
âœ“ Team expertise in optimization
```

### Horizontal Scaling (Scale Out)

**Definition**: Adding more servers to distribute load.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Before                         â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚  Server 1  â”‚                    â”‚
â”‚        â”‚  1000 req/sâ”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                 â†“ Scale Out

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           After                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚      â”‚ Load Balancer   â”‚                â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚               â”‚                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚     â”‚         â”‚         â”‚               â”‚
â”‚  â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”           â”‚
â”‚  â”‚Srv 1â”‚   â”‚Srv 2â”‚   â”‚Srv 3â”‚           â”‚
â”‚  â”‚333  â”‚   â”‚333  â”‚   â”‚333  â”‚           â”‚
â”‚  â”‚req/sâ”‚   â”‚req/sâ”‚   â”‚req/sâ”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                          â”‚
â”‚  Total: 1000 req/s (can add more!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- âœ… Nearly unlimited scaling
- âœ… Better fault tolerance (redundancy)
- âœ… Cost-effective (use commodity hardware)
- âœ… No downtime for scaling
- âœ… Geographic distribution possible

**Cons:**
- âŒ Complex architecture
- âŒ Data consistency challenges
- âŒ Requires load balancing
- âŒ More moving parts to manage
- âŒ Higher operational complexity

**When to Use:**
- High-traffic systems (> 100,000 users)
- Need high availability
- Global user base
- Unpredictable growth
- Stateless services

**Real-World Example: Netflix**

```
Netflix Architecture (Simplified):
- 1000+ microservices
- Deployed across 3 AWS regions
- Auto-scaling based on demand
- Can handle 200M+ subscribers

Why horizontal scaling is essential:
âœ“ Massive global scale
âœ“ Variable load (evening peak)
âœ“ Need high availability (99.99%+)
âœ“ Geographic distribution
```

### Comparison Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect           â”‚ Vertical          â”‚ Horizontal         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Complexity       â”‚ Low               â”‚ High               â”‚
â”‚ Cost (small)     â”‚ Low               â”‚ Higher             â”‚
â”‚ Cost (large)     â”‚ Very High         â”‚ Moderate           â”‚
â”‚ Scalability Limitâ”‚ Hardware limits   â”‚ Nearly unlimited   â”‚
â”‚ Availability     â”‚ Single point fail â”‚ High (redundancy)  â”‚
â”‚ Consistency      â”‚ Easy              â”‚ Challenging        â”‚
â”‚ Downtime         â”‚ Required          â”‚ Zero downtime      â”‚
â”‚ Implementation   â”‚ Hours             â”‚ Weeks/Months       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.3 Load Balancing

Load balancers distribute traffic across multiple servers to ensure no single server is overwhelmed.

### Load Balancer Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4 (Transport Layer) Load Balancing          â”‚
â”‚  - Works with TCP/UDP                              â”‚
â”‚  - Routes based on IP address and port             â”‚
â”‚  - Very fast (no packet inspection)                â”‚
â”‚  - Example: AWS Network Load Balancer (NLB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 7 (Application Layer) Load Balancing        â”‚
â”‚  - Works with HTTP/HTTPS                           â”‚
â”‚  - Routes based on URL, headers, cookies           â”‚
â”‚  - Slower (inspects content)                       â”‚
â”‚  - More intelligent routing                        â”‚
â”‚  - Example: AWS Application Load Balancer (ALB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Load Balancing Algorithms

#### 1. Round Robin

**How it works**: Distribute requests sequentially to each server.

```
Request Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Req 1 â†’ Server 1
     â”‚ Req 2 â†’ Server 2
     â”‚ Req 3 â†’ Server 3
     â”‚ Req 4 â†’ Server 1
     â”‚ Req 5 â†’ Server 2
     â””â”€ Req 6 â†’ Server 3
```

**Implementation:**
```javascript
class RoundRobinLoadBalancer {
  constructor(servers) {
    this.servers = servers;
    this.currentIndex = 0;
  }

  getNextServer() {
    const server = this.servers[this.currentIndex];
    this.currentIndex = (this.currentIndex + 1) % this.servers.length;
    return server;
  }
}

// Usage
const lb = new RoundRobinLoadBalancer([
  'server1.example.com',
  'server2.example.com',
  'server3.example.com'
]);

console.log(lb.getNextServer()); // server1
console.log(lb.getNextServer()); // server2
console.log(lb.getNextServer()); // server3
console.log(lb.getNextServer()); // server1
```

**When to use:**
- âœ… Servers have similar capacity
- âœ… Requests have similar processing time
- âœ… Simple, predictable distribution

**When NOT to use:**
- âŒ Servers have different capacities
- âŒ Requests vary significantly in processing time

#### 2. Weighted Round Robin

**How it works**: Servers with higher capacity receive more requests.

```
Server Capacities:
- Server 1: Weight 3 (powerful)
- Server 2: Weight 1 (standard)

Distribution:
Req 1 â†’ Server 1
Req 2 â†’ Server 1
Req 3 â†’ Server 1
Req 4 â†’ Server 2
Req 5 â†’ Server 1 (cycle repeats)
```

**Implementation:**
```javascript
class WeightedRoundRobinLoadBalancer {
  constructor(servers) {
    // servers = [{ url: 'server1', weight: 3 }, ...]
    this.servers = [];

    // Expand servers based on weight
    servers.forEach(server => {
      for (let i = 0; i < server.weight; i++) {
        this.servers.push(server.url);
      }
    });

    this.currentIndex = 0;
  }

  getNextServer() {
    const server = this.servers[this.currentIndex];
    this.currentIndex = (this.currentIndex + 1) % this.servers.length;
    return server;
  }
}

// Usage
const lb = new WeightedRoundRobinLoadBalancer([
  { url: 'server1.example.com', weight: 5 },  // Powerful
  { url: 'server2.example.com', weight: 3 },  // Medium
  { url: 'server3.example.com', weight: 1 }   // Basic
]);
```

#### 3. Least Connections

**How it works**: Route to server with fewest active connections.

```
State at time T:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Server 1: 10 active connections     â”‚
â”‚ Server 2: 5 active connections  â†   â”‚ Choose this!
â”‚ Server 3: 15 active connections     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**
```javascript
class LeastConnectionsLoadBalancer {
  constructor(servers) {
    this.servers = servers.map(url => ({
      url,
      activeConnections: 0
    }));
  }

  getNextServer() {
    // Find server with least connections
    const server = this.servers.reduce((min, current) =>
      current.activeConnections < min.activeConnections ? current : min
    );

    return server.url;
  }

  incrementConnections(serverUrl) {
    const server = this.servers.find(s => s.url === serverUrl);
    if (server) server.activeConnections++;
  }

  decrementConnections(serverUrl) {
    const server = this.servers.find(s => s.url === serverUrl);
    if (server && server.activeConnections > 0) {
      server.activeConnections--;
    }
  }
}

// Usage
const lb = new LeastConnectionsLoadBalancer([
  'server1.example.com',
  'server2.example.com',
  'server3.example.com'
]);

async function handleRequest(req) {
  const server = lb.getNextServer();
  lb.incrementConnections(server);

  try {
    const response = await forwardToServer(server, req);
    return response;
  } finally {
    lb.decrementConnections(server);
  }
}
```

**When to use:**
- âœ… Requests have varying processing times
- âœ… Long-lived connections (WebSockets)
- âœ… Servers have similar capacity

#### 4. IP Hash

**How it works**: Route based on client IP address (same client â†’ same server).

```
hash(ClientIP) % NumberOfServers = ServerIndex

Client 192.168.1.100 â†’ hash % 3 = 1 â†’ Server 1
Client 192.168.1.101 â†’ hash % 3 = 2 â†’ Server 2
Client 192.168.1.102 â†’ hash % 3 = 0 â†’ Server 0

Same client always goes to same server!
```

**Implementation:**
```javascript
class IPHashLoadBalancer {
  constructor(servers) {
    this.servers = servers;
  }

  getNextServer(clientIP) {
    const hash = this.hashIP(clientIP);
    const serverIndex = hash % this.servers.length;
    return this.servers[serverIndex];
  }

  hashIP(ip) {
    // Simple hash function (production: use better hash)
    let hash = 0;
    for (let i = 0; i < ip.length; i++) {
      hash = ((hash << 5) - hash) + ip.charCodeAt(i);
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash);
  }
}

// Usage
const lb = new IPHashLoadBalancer([
  'server1.example.com',
  'server2.example.com',
  'server3.example.com'
]);

app.use((req, res, next) => {
  const clientIP = req.ip;
  const server = lb.getNextServer(clientIP);

  // Forward to selected server
  proxy.web(req, res, { target: server });
});
```

**When to use:**
- âœ… Need session stickiness
- âœ… Server-side caching per user
- âœ… Consistent routing required

**Problem: Server Addition/Removal**
```
3 Servers: hash % 3
Client A â†’ Server 1

Add 1 server (now 4):
Client A â†’ hash % 4 â†’ Server 2 (changed!)

Solution: Consistent Hashing (see below)
```

#### 5. Consistent Hashing

**Problem with simple hash**: Adding/removing servers redistributes ALL keys.

**Solution**: Hash both servers and keys onto a ring.

```
Consistent Hash Ring:

         Server A (hash=0)
              â†“
        0â”€â”€â”€â”€â”€â”€â”€â”€â†’ 90
        â†‘          â†“
      360 â†â”€â”€â”€â”€â”€â”€â”€ 180
        â†‘          â†“
      270 â†â”€â”€â”€â”€â”€â”€â†’ 180
              â†‘
         Server B (hash=90)
         Server C (hash=180)

Key "user123" â†’ hash=45
Clockwise to next server â†’ Server B

Add Server D (hash=135):
Only keys between 90-135 move to D
Keys at other positions unchanged!
```

**Implementation:**
```javascript
class ConsistentHashLoadBalancer {
  constructor(servers, virtualNodes = 150) {
    this.ring = new Map();
    this.servers = servers;
    this.virtualNodes = virtualNodes;

    // Add servers to ring with virtual nodes
    servers.forEach(server => {
      for (let i = 0; i < virtualNodes; i++) {
        const hash = this.hash(`${server}:${i}`);
        this.ring.set(hash, server);
      }
    });

    // Sort ring by hash value
    this.sortedHashes = Array.from(this.ring.keys()).sort((a, b) => a - b);
  }

  getNextServer(key) {
    const hash = this.hash(key);

    // Find first server clockwise from hash
    for (const serverHash of this.sortedHashes) {
      if (serverHash >= hash) {
        return this.ring.get(serverHash);
      }
    }

    // Wrap around to first server
    return this.ring.get(this.sortedHashes[0]);
  }

  hash(key) {
    // Simple hash (production: use MD5 or SHA1)
    let hash = 0;
    for (let i = 0; i < key.length; i++) {
      hash = ((hash << 5) - hash) + key.charCodeAt(i);
      hash = hash & hash;
    }
    return Math.abs(hash);
  }

  addServer(server) {
    for (let i = 0; i < this.virtualNodes; i++) {
      const hash = this.hash(`${server}:${i}`);
      this.ring.set(hash, server);
    }
    this.sortedHashes = Array.from(this.ring.keys()).sort((a, b) => a - b);
  }

  removeServer(server) {
    for (let i = 0; i < this.virtualNodes; i++) {
      const hash = this.hash(`${server}:${i}`);
      this.ring.delete(hash);
    }
    this.sortedHashes = Array.from(this.ring.keys()).sort((a, b) => a - b);
  }
}

// Usage
const lb = new ConsistentHashLoadBalancer([
  'server1.example.com',
  'server2.example.com',
  'server3.example.com'
]);

// Route request
const server = lb.getNextServer('user:12345');

// Add new server (minimal redistribution)
lb.addServer('server4.example.com');
```

**Benefits:**
- âœ… Adding server: Only ~1/N keys redistributed
- âœ… Removing server: Only that server's keys move
- âœ… Predictable, consistent routing

**Real-World Usage:**
- Amazon DynamoDB (data partitioning)
- Memcached (cache distribution)
- Cassandra (data distribution)

### Load Balancer Architecture

**Single Load Balancer (Not Recommended):**
```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚Load Balancer â”‚ â† Single Point of Failure!
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         â”‚         â”‚
     Server1   Server2   Server3
```

**Highly Available Load Balancer (Recommended):**
```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   DNS (Route53) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚ LB Active â”‚      â”‚ LB Standbyâ”‚
   â”‚ (Primary) â”‚â—„â”€â”€â”€â”€â–ºâ”‚ (Backup)  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              Heartbeat
        â”‚
   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
   â”‚    â”‚     â”‚
Server1 Server2 Server3
```

---

## 2.4 Content Delivery Network (CDN)

CDN is a geographically distributed network of servers that cache content close to users.

### How CDN Works

```
Without CDN:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User in  â”‚  â”€â”€â”€â”€ 5000 km â”€â”€â”€â”€â”€â”€â”€â†’   â”‚ Server   â”‚
â”‚ Asia     â”‚  â†â”€â”€â”€ 500ms latency â”€â”€â”€  â”‚ in USA   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With CDN:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User in  â”‚  â”€â”€â†’   â”‚ CDN Edge in  â”‚  (Cache Hit)
â”‚ Asia     â”‚  â†â”€â”€   â”‚ Tokyo (50ms) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ (Cache Miss)
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Origin Serverâ”‚
                    â”‚ in USA       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CDN Architecture

```
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Origin Server    â”‚
             â”‚ (Your backend)   â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚           â”‚           â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ CDN Edge â”‚ â”‚CDN Edgeâ”‚ â”‚CDN Edgeâ”‚
    â”‚ US-East  â”‚ â”‚EU-West â”‚ â”‚Asia-SE â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚           â”‚           â”‚
    Users in US  Users in EU  Users in Asia
```

### What to Cache on CDN

```
âœ… Static Assets:
- Images (JPEG, PNG, WebP)
- Videos
- CSS, JavaScript files
- Fonts
- PDFs, documents

âœ… Semi-Static Content:
- HTML pages (with short TTL)
- API responses (for public data)
- RSS feeds

âŒ Don't Cache:
- User-specific data
- Real-time data
- Authentication tokens
- Checkout/payment pages
```

### CDN Cache Headers

```javascript
// Express.js example: Setting cache headers
app.get('/api/products', (req, res) => {
  // Cache for 5 minutes
  res.set('Cache-Control', 'public, max-age=300');
  res.json(products);
});

app.get('/images/:filename', (req, res) => {
  // Cache for 1 year (immutable)
  res.set('Cache-Control', 'public, max-age=31536000, immutable');
  res.sendFile(filename);
});

app.get('/api/user/profile', (req, res) => {
  // Don't cache user-specific data
  res.set('Cache-Control', 'private, no-cache, no-store, must-revalidate');
  res.json(userProfile);
});
```

### Cache Invalidation

**Problem**: How to update cached content when origin changes?

**Solutions:**

1. **Time-Based (TTL)**
```
Set Cache-Control: max-age=3600

Content expires after 1 hour, CDN fetches fresh copy
```

2. **Versioned URLs**
```
Old: /static/app.js
New: /static/app.v2.js  or  /static/app.js?v=2

Each version is a new URL, cached separately
```

3. **Cache Purge (Invalidation API)**
```javascript
// CloudFront invalidation
const cloudfront = new AWS.CloudFront();

await cloudfront.createInvalidation({
  DistributionId: 'DISTRIBUTION_ID',
  InvalidationBatch: {
    Paths: {
      Quantity: 1,
      Items: ['/images/logo.png']
    },
    CallerReference: Date.now().toString()
  }
}).promise();
```

### Real-World Example: Netflix

```
Netflix CDN Strategy:

1. Own CDN (Open Connect)
   - 1000+ servers in ISP data centers
   - Content pre-positioned during off-peak
   - Serves 95%+ of traffic

2. Multi-tiered Caching:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Origin       â”‚ (Master copy)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Regional Cacheâ”‚ (Popular content)
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Edge Cache    â”‚ (All content)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Adaptive Bitrate Streaming:
   - Multiple quality levels cached
   - Client selects based on bandwidth
```

**Results:**
- 200M+ subscribers served globally
- < 100ms start time for most users
- 99.99% availability
- Massive cost savings (vs third-party CDN)

---

## 2.5 Caching Strategies

Caching is the most effective way to improve performance and scalability.

### Where to Cache

```
Browser â†’ CDN â†’ Load Balancer â†’ App Server â†’ Database

Each layer can cache:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser Cache (localStorage, cookies)            â”‚
â”‚ - User preferences, auth tokens                  â”‚
â”‚ - Reduces server requests                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CDN Cache (edge servers)                         â”‚
â”‚ - Static assets, images, videos                  â”‚
â”‚ - Closest to user (lowest latency)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Cache (Redis, Memcached)             â”‚
â”‚ - API responses, computed results                â”‚
â”‚ - Shared across app servers                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database Cache (query cache, buffer pool)        â”‚
â”‚ - Frequently accessed rows                       â”‚
â”‚ - Reduces disk I/O                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Application-Level Caching

**Redis Example:**

```javascript
const redis = require('redis');
const client = redis.createClient();

class ProductService {
  async getProduct(productId) {
    // Try cache first
    const cached = await client.get(`product:${productId}`);
    if (cached) {
      console.log('Cache hit!');
      return JSON.parse(cached);
    }

    // Cache miss - fetch from database
    console.log('Cache miss - querying database');
    const product = await db.products.findById(productId);

    // Store in cache for 1 hour
    await client.setex(
      `product:${productId}`,
      3600,
      JSON.stringify(product)
    );

    return product;
  }

  async updateProduct(productId, updates) {
    // Update database
    await db.products.update(productId, updates);

    // Invalidate cache
    await client.del(`product:${productId}`);
  }
}
```

### Cache Eviction Policies

When cache is full, which items to remove?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LRU (Least Recently Used)                       â”‚
â”‚ - Remove items not accessed recently            â”‚
â”‚ - Best for most use cases                       â”‚
â”‚ - Redis default                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LFU (Least Frequently Used)                     â”‚
â”‚ - Remove items accessed least often             â”‚
â”‚ - Good for popular items                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FIFO (First In First Out)                       â”‚
â”‚ - Remove oldest items first                     â”‚
â”‚ - Simple but less effective                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TTL (Time To Live)                              â”‚
â”‚ - Items expire after set time                   â”‚
â”‚ - Combines with other policies                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cache Stampede Problem

**Problem**: Cache expires, many requests hit database simultaneously.

```
Time: T0 - Cache expires
Time: T1 - 1000 requests arrive simultaneously
         - All see cache miss
         - All query database
         - Database overloaded! ğŸ’¥
```

**Solution 1: Locking**
```javascript
class CacheWithLock {
  async get(key, fetchFunction) {
    // Try cache
    const cached = await redis.get(key);
    if (cached) return JSON.parse(cached);

    // Acquire lock
    const lockKey = `lock:${key}`;
    const locked = await redis.set(lockKey, '1', 'NX', 'EX', 10);

    if (locked) {
      // This request won the lock - fetch data
      try {
        const data = await fetchFunction();
        await redis.setex(key, 3600, JSON.stringify(data));
        return data;
      } finally {
        await redis.del(lockKey);
      }
    } else {
      // Another request is fetching - wait and retry
      await new Promise(resolve => setTimeout(resolve, 100));
      return this.get(key, fetchFunction);
    }
  }
}
```

**Solution 2: Early Recomputation**
```javascript
// Refresh cache before expiry
class CacheWithRefresh {
  async get(key, fetchFunction, ttl = 3600) {
    const cached = await redis.get(key);

    if (cached) {
      const data = JSON.parse(cached);

      // Check if cache is expiring soon (within 10% of TTL)
      const ttlRemaining = await redis.ttl(key);
      if (ttlRemaining < ttl * 0.1) {
        // Refresh in background
        this.refreshAsync(key, fetchFunction, ttl);
      }

      return data;
    }

    // Cache miss - fetch synchronously
    const data = await fetchFunction();
    await redis.setex(key, ttl, JSON.stringify(data));
    return data;
  }

  async refreshAsync(key, fetchFunction, ttl) {
    // Don't await - runs in background
    fetchFunction().then(data => {
      redis.setex(key, ttl, JSON.stringify(data));
    });
  }
}
```

---

## 2.6 Real-World Scalability Case Studies

### Case Study 1: Twitter's Fanout Architecture

**Problem**: When @elonmusk tweets (100M+ followers), how to deliver to all timelines?

**Naive Approach (Pull Model):**
```
User opens app:
1. Query: "Get tweets from everyone I follow"
2. SELECT * FROM tweets
   WHERE user_id IN (following_list)
   ORDER BY timestamp DESC
   LIMIT 20

Problem: Query is slow (joins many users)
Can't scale to millions of users
```

**Twitter's Solution (Push Model - Fanout):**
```
When user tweets:
1. Write tweet to database
2. Fanout: Push to followers' timelines

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User tweets     â”‚
â”‚ "Hello world!"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Fanout Service        â”‚
    â”‚ (Async queue workers) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€ Push to Follower 1 timeline (Redis)
         â”œâ”€ Push to Follower 2 timeline (Redis)
         â”œâ”€ Push to Follower 3 timeline (Redis)
         â””â”€ ... (100M followers)

When follower opens app:
- Read from pre-computed timeline (Redis)
- Super fast! (< 10ms)
```

**Hybrid Approach** (for celebrities):
```
Regular users (< 1M followers):
- Use push model (fanout on write)

Celebrities (> 1M followers):
- DON'T fanout on write (too slow)
- Merge tweets in real-time on read

Timeline = Pre-computed timeline + Celebrity tweets
```

**Implementation Sketch:**
```javascript
class TwitterTimeline {
  async postTweet(userId, content) {
    // Save tweet
    const tweet = await db.tweets.create({ userId, content });

    // Check if user is celebrity
    const followerCount = await db.users.getFollowerCount(userId);

    if (followerCount < 1000000) {
      // Regular user - fanout
      await this.fanoutToFollowers(userId, tweet);
    }
    // Celebrities: skip fanout (too expensive)

    return tweet;
  }

  async fanoutToFollowers(userId, tweet) {
    // Get followers (paginated)
    const followers = await db.users.getFollowers(userId);

    // Queue fanout jobs
    for (const followerId of followers) {
      await queue.add('fanout', {
        followerId,
        tweet
      });
    }
  }

  async getTimeline(userId) {
    // Get pre-computed timeline
    const timeline = await redis.lrange(`timeline:${userId}`, 0, 19);

    // Get celebrities user follows
    const celebrities = await db.users.getFollowedCelebrities(userId);

    // Fetch recent celebrity tweets
    const celebrityTweets = await db.tweets.find({
      userId: { $in: celebrities },
      timestamp: { $gt: Date.now() - 86400000 } // Last 24h
    });

    // Merge and sort
    const merged = [...timeline, ...celebrityTweets]
      .sort((a, b) => b.timestamp - a.timestamp)
      .slice(0, 20);

    return merged;
  }
}
```

### Case Study 2: Instagram's Photo Storage

**Challenge**: Store billions of photos efficiently.

**Requirements:**
- 100M photos uploaded per day
- Average size: 300 KB
- Storage: 100M Ã— 300 KB = 30 TB per day!
- 5 years: 30 TB Ã— 365 Ã— 5 = 55 PB

**Solution: Tiered Storage**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hot Storage (Recent photos - last 30 days) â”‚
â”‚ - Fast SSD storage                         â”‚
â”‚ - 100M Ã— 30 Ã— 300KB = 900 TB              â”‚
â”‚ - Cost: High, but accessed frequently     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (aging)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Warm Storage (30 days - 1 year)           â”‚
â”‚ - Standard HDD storage                     â”‚
â”‚ - Accessed occasionally                    â”‚
â”‚ - Cost: Medium                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (aging)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cold Storage (> 1 year)                    â”‚
â”‚ - Glacier / archival storage               â”‚
â”‚ - Rarely accessed                          â”‚
â”‚ - Cost: Very low                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Image Processing Pipeline:**

```
1. User uploads photo (5 MB)
         â†“
2. Resize & optimize
   - Original: 5 MB
   - Large: 1920px â†’ 500 KB
   - Medium: 1080px â†’ 200 KB
   - Thumbnail: 320px â†’ 50 KB
   Total: 5.75 MB
         â†“
3. Store in CDN
   - Serve optimized version based on device
   - Mobile â†’ Thumbnail
   - Desktop â†’ Large
         â†“
4. Lazy loading
   - Load thumbnails first
   - Load full resolution on click
```

**Cost Savings:**
```
Before optimization:
- 100M photos/day Ã— 5 MB = 500 TB/day
- AWS S3: $0.023/GB = $11,500/day

After optimization:
- Serve 80% from thumbnails (50 KB)
- CDN cache hit rate: 95%
- Storage: 100M Ã— 575 KB = 57.5 TB/day
- Cost reduced by ~85%
```

---

## 2.7 Key Takeaways

### Scalability Principles

1. **Start Simple, Scale When Needed**
   - Don't over-engineer early
   - Vertical scaling first, horizontal when necessary
   - Measure before optimizing

2. **Cache Aggressively**
   - Cache at every layer
   - Cache hit rate > 80% for best results
   - Invalidate carefully

3. **Design for Failure**
   - Load balancers should be redundant
   - No single points of failure
   - Health checks and automatic failover

4. **Optimize for Common Case**
   - Twitter: Optimize reads (fanout on write)
   - Instagram: Optimize delivery (CDN, lazy load)
   - Identify your bottleneck first

### Performance Metrics to Track

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Good         â”‚ Excellent   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ API Latency (p50)   â”‚ < 200ms      â”‚ < 100ms     â”‚
â”‚ API Latency (p99)   â”‚ < 1000ms     â”‚ < 500ms     â”‚
â”‚ Cache Hit Rate      â”‚ > 80%        â”‚ > 95%       â”‚
â”‚ Availability        â”‚ 99.9%        â”‚ 99.99%      â”‚
â”‚ Error Rate          â”‚ < 1%         â”‚ < 0.1%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common Scaling Mistakes

âŒ **Premature Optimization**: Building for 1M users when you have 100
âœ… **Do**: Start simple, measure, then scale

âŒ **Caching Everything**: Including user-specific or real-time data
âœ… **Do**: Cache static and semi-static content only

âŒ **No Monitoring**: Can't improve what you don't measure
âœ… **Do**: Monitor latency, throughput, error rates

âŒ **Ignoring Database**: Scaling app servers but database is bottleneck
âœ… **Do**: Profile queries, add indexes, consider read replicas

---

## 2.8 Interview Questions

### Basic:
1. What's the difference between vertical and horizontal scaling?
2. What is a load balancer and why do we need it?
3. Explain how CDN works.

### Intermediate:
1. Compare round-robin and least connections load balancing.
2. What is consistent hashing and when would you use it?
3. How do you handle cache invalidation?

### Advanced:
1. Design Twitter's timeline delivery system for 500M users.
2. How would you scale a write-heavy application?
3. Explain the trade-offs between fanout-on-write vs fanout-on-read.

---

**Next Chapter:** [Chapter 3: Database Design & Patterns](./SystemDesign-03-Database.md)

In the next chapter, we'll dive deep into:
- SQL vs NoSQL trade-offs
- Database sharding strategies
- Replication patterns
- Indexing optimization
- Real-world data modeling
